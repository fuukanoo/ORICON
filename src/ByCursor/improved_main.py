"""
æ”¹å–„ã•ã‚ŒãŸORICONãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å€‹äººãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æã€æ¡ä»¶ä»˜ãVAEã€
é«˜åº¦ãªæœ€é©è¼¸é€ã‚’çµ±åˆã—ãŸåŒ…æ‹¬çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³
"""

import sys
import os
# ORICONãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

import torch
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional
from sklearn.preprocessing import StandardScaler

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from utils.utils import read_data, set_seed
from utils.logger import init_logger
from utils.args import get_args
from src.Optimal_transportation.ot import run_ot_for_candidate

# æ–°è¦æ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.ByCursor.data_preprocessing.user_segmentation import UserSegmentAnalyzer
from src.ByCursor.modeling.conditional_vae import SegmentBasedServiceGenerator, CVAETrainer

class ImprovedORICONPipeline:
    """
    æ”¹å–„ã•ã‚ŒãŸORICONãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    å¾“æ¥ã®å˜ç´”ãªã‚µãƒ¼ãƒ“ã‚¹é›†è¨ˆãƒ¬ãƒ™ãƒ«åˆ†æã‹ã‚‰ã€
    å€‹äººãƒ¬ãƒ™ãƒ«ãƒ»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®é«˜ç²¾åº¦åˆ†æã‚·ã‚¹ãƒ†ãƒ ã¸é€²åŒ–
    """
    
    def __init__(self, config, args):
        self.config = config
        self.args = args
        self.logger = init_logger(config.project_name, level=logging.INFO)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨
        self.df = None
        self.format_df = None
        self.user_segments = None
        self.segment_profiles = None
        self.generated_services = {}
        self.ot_results = []
        
    def load_and_preprocess_data(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        self.logger.info("=== Phase 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç† ===")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.df, self.format_df = read_data()
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {self.df.shape}")
        
        # åŸºæœ¬çµ±è¨ˆ
        self.logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(self.df)}")
        self.logger.info(f"ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨åˆ†å¸ƒ: {self.df['SQ6_2'].value_counts().head()}")
        
    def perform_user_segmentation(self) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ"""
        self.logger.info("=== Phase 2: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ ===")
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æå™¨åˆæœŸåŒ–
        analyzer = UserSegmentAnalyzer(self.df, self.format_df)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        clustering_result = analyzer.perform_clustering(
            n_clusters=self.args.n_segments if hasattr(self.args, 'n_segments') else 5,
            method='kmeans'
        )
        
        self.logger.info(f"è­˜åˆ¥ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {clustering_result['n_clusters']}")
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        self.segment_profiles = analyzer.analyze_segment_profiles()
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ä¿å­˜
        self.user_segments = analyzer.segments
        
        # çµæœå‡ºåŠ›
        for segment_name, profile in self.segment_profiles.items():
            self.logger.info(f"{segment_name}: {profile['size']}å ({profile['percentage']:.1f}%)")
            
        # å¯è¦–åŒ–ãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        try:
            analyzer.visualize_segments(f"{self.config.shared_data_path}/user_segments.png")
            analyzer.export_segments(f"{self.config.shared_data_path}/user_segments")
            self.logger.info("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.warning(f"å¯è¦–åŒ–ãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_segment_based_services(self) -> None:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆ"""
        self.logger.info("=== Phase 3: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆ ===")
        
        if self.user_segments is None:
            raise ValueError("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ å®šç¾©ï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ï¼‰
        numeric_cols = self.user_segments.select_dtypes(include=[np.number]).columns
        feature_columns = [col for col in numeric_cols if col != 'cluster']
        
        self.logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(feature_columns)}")
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆå™¨
        generator = SegmentBasedServiceGenerator(self.user_segments)
        
        # CVAEè¨“ç·´
        self.logger.info("CVAEè¨“ç·´é–‹å§‹...")
        trainer = generator.train_cvae(
            feature_columns,
            hidden_dim=self.args.cvae_hidden_dim if hasattr(self.args, 'cvae_hidden_dim') else 128,
            latent_dim=self.args.cvae_latent_dim if hasattr(self.args, 'cvae_latent_dim') else 32,
            epochs=self.args.cvae_epochs if hasattr(self.args, 'cvae_epochs') else 100,
            batch_size=self.args.cvae_batch_size if hasattr(self.args, 'cvae_batch_size') else 32,
            lr=self.args.cvae_lr if hasattr(self.args, 'cvae_lr') else 1e-3,
            beta=self.args.cvae_beta if hasattr(self.args, 'cvae_beta') else 1.0
        )
        
        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‘ã‘æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆ
        n_services_per_segment = self.args.n_services_per_segment if hasattr(self.args, 'n_services_per_segment') else 10
        
        for segment_id in self.user_segments['cluster'].unique():
            if segment_id == -1:  # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¯é™¤å¤–
                continue
                
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{segment_id}å‘ã‘æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆ...")
            
            generated = generator.generate_segment_specific_services(
                trainer, segment_id, n_services_per_segment
            )
            
            # åˆ†æçµæœ
            analysis = generator.analyze_generated_services(
                generated, feature_columns, segment_id
            )
            
            self.generated_services[segment_id] = {
                'services': generated,
                'feature_columns': feature_columns,
                'analysis': analysis
            }
            
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{segment_id}: {len(generated)}å€‹ã®æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆå®Œäº†")
    
    def perform_advanced_optimal_transport(self) -> None:
        """é«˜åº¦ãªæœ€é©è¼¸é€åˆ†æ"""
        self.logger.info("=== Phase 4: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æœ€é©è¼¸é€åˆ†æ ===")
        
        if not self.generated_services:
            raise ValueError("æ–°ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã«æœ€é©è¼¸é€åˆ†æ
        all_results = []
        
        for segment_id, segment_data in self.generated_services.items():
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{segment_id}ã®æœ€é©è¼¸é€åˆ†æ...")
            
            generated_services = segment_data['services']
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹åŸ‹ã‚è¾¼ã¿ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            segment_users = self.user_segments[self.user_segments['cluster'] == segment_id]
            
            if len(segment_users) < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
                self.logger.warning(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{segment_id}ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            feature_cols = segment_data['feature_columns']
            X_curr_segment = segment_users[feature_cols].values
            Y_fut_segment = generated_services
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_curr_scaled = scaler.fit_transform(X_curr_segment)
            Y_fut_scaled = scaler.transform(Y_fut_segment)
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥è³ªé‡åˆ†å¸ƒ
            segment_size = len(segment_users)
            total_users = len(self.user_segments)
            segment_ratio = segment_size / total_users
            
            mass_curr = np.ones(X_curr_scaled.shape[0]) / X_curr_scaled.shape[0] * segment_ratio
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            segment_profile = self.segment_profiles.get(f'cluster_{segment_id}', {})
            
            # æº€è¶³åº¦ã«åŸºã¥ãéãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªé‡èª¿æ•´
            satisfaction_metrics = segment_profile.get('satisfaction_metrics', {})
            avg_satisfaction = satisfaction_metrics.get('total_satisfaction', {}).get('mean', 5.0)
            
            # æº€è¶³åº¦ãŒä½ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯æ–°ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®ç§»è¡Œç¢ºç‡ãŒé«˜ã„
            nonuser_mass = self.args.nonuser_mass * (10 - avg_satisfaction) / 5.0
            residual_mass = self.args.residual_mass * avg_satisfaction / 10.0
            
            # å„æ–°ã‚µãƒ¼ãƒ“ã‚¹å€™è£œã«å¯¾ã—ã¦OTå®Ÿè¡Œ
            for service_idx in range(len(Y_fut_segment)):
                try:
                    result = run_ot_for_candidate(
                        X_curr_scaled, Y_fut_scaled, service_idx,
                        mass_curr, nonuser_mass, residual_mass,
                        eps=0.2, tau=0.1,
                        total_market_size=self.args.total_market_size,
                        arpu_list=self.args.arpu_list
                    )
                    
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±è¿½åŠ 
                    result['segment_id'] = segment_id
                    result['segment_size'] = segment_size
                    result['segment_ratio'] = segment_ratio
                    result['avg_satisfaction'] = avg_satisfaction
                    result['service_id'] = f"segment_{segment_id}_service_{service_idx}"
                    
                    all_results.append(result)
                    
                except Exception as e:
                    self.logger.error(f"OTã‚¨ãƒ©ãƒ¼ (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{segment_id}, ã‚µãƒ¼ãƒ“ã‚¹{service_idx}): {e}")
        
        self.ot_results = all_results
        self.logger.info(f"æœ€é©è¼¸é€åˆ†æå®Œäº†: {len(all_results)}ä»¶ã®çµæœ")
    
    def analyze_and_export_results(self) -> None:
        """çµæœåˆ†æãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        self.logger.info("=== Phase 5: çµæœåˆ†æãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ===")
        
        if not self.ot_results:
            self.logger.warning("åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # çµæœã‚’DataFrameã«å¤‰æ›
        df_results = pd.DataFrame(self.ot_results)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼
        segment_summary = df_results.groupby('segment_id').agg({
            'total_inflow': ['mean', 'max', 'std'],
            'nonuser_ratio': ['mean', 'max', 'std'],
            'estimated_sales': ['mean', 'max', 'std'],
            'BlueOceanScore': ['mean', 'max', 'std']
        }).round(2)
        
        self.logger.info("=== ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æçµæœ ===")
        print(segment_summary)
        
        # æœ€å„ªç§€ã‚µãƒ¼ãƒ“ã‚¹å€™è£œï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ï¼‰
        best_services = df_results.loc[df_results.groupby('segment_id')['estimated_sales'].idxmax()]
        
        self.logger.info("=== ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æœ€å„ªç§€æ–°ã‚µãƒ¼ãƒ“ã‚¹å€™è£œ ===")
        for _, service in best_services.iterrows():
            self.logger.info(
                f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{service['segment_id']}: "
                f"æ¨å®šå£²ä¸Š{service['estimated_sales']:,.0f}å††, "
                f"éãƒ¦ãƒ¼ã‚¶ãƒ¼ç²å¾—ç‡{service['nonuser_ratio']:.1%}, "
                f"BlueOceanã‚¹ã‚³ã‚¢{service['BlueOceanScore']:.4f}"
            )
        
        # çµæœä¿å­˜
        output_path = f"{self.config.results_data_path}/improved_ot_results.csv"
        df_results.to_csv(output_path, index=False)
        
        summary_path = f"{self.config.results_data_path}/segment_summary.csv"
        segment_summary.to_csv(summary_path)
        
        best_services_path = f"{self.config.results_data_path}/best_services_by_segment.csv"
        best_services.to_csv(best_services_path, index=False)
        
        self.logger.info(f"æ”¹å–„ã•ã‚ŒãŸåˆ†æçµæœã‚’ä¿å­˜: {output_path}")
    
    def run_full_pipeline(self) -> None:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ æ”¹å–„ã•ã‚ŒãŸORICONãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        
        try:
            # ã‚·ãƒ¼ãƒ‰è¨­å®š
            set_seed(self.args.seed)
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            self.load_and_preprocess_data()
            self.perform_user_segmentation()
            self.generate_segment_based_services()
            self.perform_advanced_optimal_transport()
            self.analyze_and_export_results()
            
            self.logger.info("âœ… æ”¹å–„ã•ã‚ŒãŸORICONãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise


class ImprovedConfig:
    """æ”¹å–„ã•ã‚ŒãŸè¨­å®šã‚¯ãƒ©ã‚¹"""
    shared_data_path = "./data/shared"
    results_data_path = "./data/results"
    project_name = "ORICON_Improved"


def create_improved_args():
    """æ”¹å–„ã•ã‚ŒãŸargparseã®ä½œæˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Improved ORICON Pipeline")
    
    # åŸºæœ¬è¨­å®š
    parser.add_argument("--seed", type=int, default=42)
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
    parser.add_argument("--n_segments", type=int, default=5)
    
    # CVAEè¨­å®š
    parser.add_argument("--cvae_hidden_dim", type=int, default=128)
    parser.add_argument("--cvae_latent_dim", type=int, default=32)
    parser.add_argument("--cvae_epochs", type=int, default=100)
    parser.add_argument("--cvae_batch_size", type=int, default=32)
    parser.add_argument("--cvae_lr", type=float, default=1e-3)
    parser.add_argument("--cvae_beta", type=float, default=1.0)
    
    # ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿæˆ
    parser.add_argument("--n_services_per_segment", type=int, default=10)
    
    # OTè¨­å®š
    parser.add_argument("--nonuser_mass", type=float, default=0.1)
    parser.add_argument("--residual_mass", type=float, default=0.1)
    parser.add_argument("--total_market_size", type=int, default=1000000)
    parser.add_argument("--arpu_list", type=list, default=[1200, 1500, 1000, 1300, 1600])
    
    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # å¼•æ•°ãƒ»è¨­å®š
    args = create_improved_args()
    config = ImprovedConfig()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = ImprovedORICONPipeline(config, args)
    pipeline.run_full_pipeline()


if __name__ == "__main__":
    main() 