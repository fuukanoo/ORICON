import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import random
import logging
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from ot.unbalanced import sinkhorn_unbalanced
import os 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.metrics import pairwise_distances 
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
import logging
logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

from src.BYOL.byol import train_byol
from src.BYOL.byol_models import BYOL, byol_loss
# from src.VAE.vae import VAE, train_vae
from src.VAE.CondBetaVAE import CondBetaVAE, vae_loss, train_vae
from src.VAE.traverse_latents_condbetavae import save_latent_traversal_plot
from src.VAE.pca_tools import run_pca, _hex_radar, save_radar_batch
from src.PCA_UMAP.visualize import visualize_pca_umap 
from utils.utils import set_seed, read_data, make_feature_df, scale_imputer
from utils.logger import init_logger
from utils.args import get_args
from utils.dataloader import ServiceDataset
from src.Optimal_transportation.ot_main import run_ot_for_candidate
from src.Optimal_transportation.utils import load_and_scale_data, calculate_mass_vectors, save_radar_charts, radar_new_services

class Config:
    """argsã§å¤‰ãˆãªã„ã‚‚ã®"""
    shared_data_path = "./data/shared"
    results_data_path = "./data/results"
    project_name = "ORICON"
    scaler_filename = "scaler.gz"
    embeddings_filename = "embeddings.npy"
    new_embeddings_filename = "emb_new.npy"
    
def compute_cost_matrix(X: np.ndarray, Y: np.ndarray, metric="euclidean") -> np.ndarray:
    """
    ã‚³ã‚¹ãƒˆè¡Œåˆ— (pairwise_distances) ã‚’è¨ˆç®—ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    """
    return pairwise_distances(X, Y, metric=metric)


def main(args, config: Config = None):
    # Logger initialization - this should be done once at the start
    logger = init_logger(config.project_name, level=logging.INFO)
    logger.info(f"{config.project_name} main process started.")

    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")
    
    set_seed(args.seed)
    logger.info(f"Random seed set to {args.seed}")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    logger.info("Loading data...")
    df, format_df = read_data()

    logger.info("Creating feature dataframe...")
    # ç‰¹å¾´é‡è¡Œåˆ—ä½œæˆ
    feat_df = make_feature_df(df, format_df)
    # æ¬ æå€¤å‡¦ç†&ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆX_scaledã®ä¿å­˜ï¼‰
    feat_df = scale_imputer(feat_df, config.shared_data_path, strategy=args.impute_strategy)
    save_feat_df_path = f"{config.shared_data_path}/feat_df.pkl"
    feat_df.to_pickle(save_feat_df_path)
    logger.info(f"Feature dataframe saved to {save_feat_df_path}")
    
    # BYOL
    if args.use_byol:
        logger.info("Preparing BYOL training...")
        X = torch.tensor(feat_df.values, dtype=torch.float)
        byol_dataset = ServiceDataset(X)
        byol_dataloader = DataLoader(byol_dataset, batch_size=8, shuffle=True)
        byol_model = BYOL(input_dim=X.shape[1]).to(device)
        byol_optimizer = torch.optim.Adam(list(byol_model.online.parameters()) + 
                                    list(byol_model.predictor.parameters()), lr=1e-3)
        logger.info(f"BYOL model created with input dimension: {X.shape[1]}")
        
        logger.info("Starting BYOL training...")
        byol_trained_model = train_byol(model=byol_model,
                        dataloader=byol_dataloader,
                        optimizer=byol_optimizer,
                        n_epochs=args.byol_n_epochs)
        logger.info("BYOL training completed.")
        
        byol_trained_model.eval()
        with torch.no_grad():
            h, _ = byol_trained_model.online(X.to(device))
            embeddings = h.cpu().numpy()
            
        np.save(f"{config.shared_data_path}/embeddings.npy", embeddings)
        np.save(f"{config.shared_data_path}/feat_columns.npy", feat_df.columns.values)
    else:
        # BYOLã‚’ä½¿ç”¨ã—ãªã„å ´åˆã®å‡¦ç†
        logger.info("BYOL not used, initializing embeddings manually...")
        embeddings = feat_df.values.astype(np.float32)  # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨
        np.save(f"{config.shared_data_path}/embeddings.npy", embeddings)       
        # if args.visualize_process:
            # logger.info("Visualizing embeddings...")
            # emb_df = pd.DataFrame(embeddings, index=feat_df.index)
            # visualize_pca_umap(emb_df, config, args, n_components=2,
                            #    title="BYOL_Embeddings", xlabel="Component 1", ylabel="Component 2")
            # Visualize embeddings (e.g., using PCA or t-SNE)
            # This part is omitted for brevity, but you can implement it as needed.
    
    # VAE
    # ğŸ“Œ è¿½è¨˜: æ¡ä»¶ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    logger.info("Building condition_onehot (ãƒšã‚¤ãƒ³=KMeans 6ã‚¯ãƒ©ã‚¹ä¾‹)â€¦")
    from sklearn.cluster import KMeans

    K = 6
    km = KMeans(n_clusters=K, random_state=42).fit(embeddings)
    cond_onehot = np.eye(K, dtype=np.float32)[km.labels_]   # shape (N, K)

    condition_dim = 0
    np.save(f"{config.shared_data_path}/cond_onehot.npy", cond_onehot)
    
    # ğŸ“Œ å¤‰æ›´: DataLoader ã‚’æ¡ä»¶ä»˜ãã«å¤‰æ›´
    cond_onehot = np.load(f"{config.shared_data_path}/cond_onehot.npy")
    vae_dataset  = TensorDataset(torch.from_numpy(embeddings).float())
    vae_dataloader = DataLoader(vae_dataset, batch_size=8, shuffle=True)
    
    # ğŸ“Œ å¤‰æ›´: VAE â†’ CondBetaVAE
    vae = CondBetaVAE(
            input_dim = embeddings.shape[1],
            hidden_dim= args.vae_hidden_dim,
            latent_dim= args.vae_latent_dim,
            condition_dim = condition_dim,
            beta = args.vae_beta
    ).to(device)
    vae_optimizer = torch.optim.Adam(vae.parameters(), lr=args.vae_lr)
    
    # vae_dataset = TensorDataset(torch.from_numpy(embeddings))
    # vae_dataloaer = DataLoader(vae_dataset, batch_size=8, shuffle=True)
    # embeddings = np.load(f"{config.shared_data_path}/embeddings.npy")
    
    # input_dim = embeddings.shape[1]
    # vae = VAE(input_dim, args.vae_hidden_dim, args.vae_latent_dim, args.vae_sigma).to(device)
    # vae_optimizer = torch.optim.Adam(vae.parameters(), lr=args.vae_lr)
    
    #TODO: 64æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾ã«ã—ã¦ã‚‹ã€‚33ã«ã™ã‚‹ãªã‚‰è¿½åŠ ã§å‡¦ç†å¿…è¦ã€‚ã‚ã‚‹ã„ã¯33ã®ã¾ã¾æœ€åˆã‹ã‚‰å‡¦ç†ã‚’ã™ã‚‹ã‹
    emb_new, z_new, recon_hist, kl_hist = train_vae(
        vae=vae,
        loader=vae_dataloader,      # DataLoader
        opt=vae_optimizer,
        dataset=vae_dataset,       # TensorDataset
        beta=args.vae_beta,
        latent_dim=args.vae_latent_dim,
        epochs=args.vae_n_epochs,
        plot_dir     = config.results_data_path
    )
    
    

    # ç”Ÿæˆçµæœã‚’ä¿å­˜ã—ã¦ OT ã§ä½¿ãˆã‚‹ã‚ˆã†ã«
    np.save(f"{config.shared_data_path}/emb_new.npy", emb_new)
    
    save_latent_traversal_plot(
        model=vae,
        device=device,
        latent_dim=args.vae_latent_dim,
        output_dir=config.results_data_path,
        feat_cols  = list(feat_df.columns)
    )
    
    # ç”Ÿæˆã—ãŸ20ä»¶ã® z ã‚’ CSV ã«å‡ºåŠ›
    z_csv_path = os.path.join(config.results_data_path, "latent_vectors_new_services.csv")
    pd.DataFrame(
        z_new,
        columns=[f"z{i}" for i in range(args.vae_latent_dim)]
    ).to_csv(z_csv_path, index=False)
    logger.info(f"latent vectors (new services) saved â†’ {z_csv_path}")
    
    # â˜… ã“ã“ã‹ã‚‰ç›¸é–¢åˆ†æã‚’è¿½åŠ  â˜… ---------------------------------
    logger.info("Computing Pearson correlation between latent Z and original featuresâ€¦")

    # â‘  æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚’ latent ç©ºé–“ã«åŸ‹ã‚è¾¼ã¿
    vae.eval()
    with torch.no_grad():
        z_existing, _ = vae.encode(torch.tensor(embeddings, dtype=torch.float32, device=device), None)
        z_existing = z_existing.cpu().numpy()


    # ---------------------------------------------------------
    # â‘¡ DataFrame åˆä½“ (Z 16åˆ— + å…ƒç‰¹å¾´33åˆ—)
    # ---------------------------------------------------------
    df_corr = pd.concat(
        [
            pd.DataFrame(z_existing, columns=[f"z{i}" for i in range(args.vae_latent_dim)]),
            feat_df.reset_index(drop=True)
        ],
        axis=1
    )

    # â‘¢ ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’è¨ˆç®—
    corr_mat = df_corr.corr(method="pearson")

    # â‘£ Z ã¨å…ƒç‰¹å¾´ã®ãƒ–ãƒ­ãƒƒã‚¯ã ã‘åˆ‡ã‚Šå‡ºã—
    corr_z_feat = corr_mat.loc[[f"z{i}" for i in range(args.vae_latent_dim)],
                            feat_df.columns]

    # â‘¤ CSV ã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¿å­˜
    corr_csv = os.path.join(config.results_data_path, "z_feature_correlation.csv")
    corr_z_feat.to_csv(corr_csv)
    logger.info(f"Zâ€“feature correlation saved â†’ {corr_csv}")

    plt.figure(figsize=(10, 6))
    sns.heatmap(corr_z_feat, cmap="coolwarm", center=0, annot=False)
    plt.title("Latent Z vs Original Feature Pearson Correlation")
    plt.tight_layout()
    corr_png = os.path.join(config.results_data_path, "z_feature_correlation.png")
    plt.savefig(corr_png); plt.close()
    logger.info(f"Correlation heatmap saved â†’ {corr_png}")

    # ---------------------------------------------------------
    # â˜… â‘¥ ãƒˆãƒƒãƒ—6è»¸ã‚’æŠ½å‡ºã—ä¿å­˜ï¼ˆ|corr| æœ€å¤§å€¤ã§é¸å®šï¼‰
    # ---------------------------------------------------------
    abs_corr      = corr_z_feat.abs()          # |corr|
    top_strength  = abs_corr.max(axis=1)       # å„ z ã®æœ€å¤§ç›¸é–¢å€¤

    chosen = []
    for z in top_strength.sort_values(ascending=False).index:
        feat = abs_corr.loc[z].idxmax()        # ãã® z ãŒä¸€ç•ªåŠ¹ãç‰¹å¾´
        if feat not in [f for _, f in chosen]: # é‡è¤‡å›é¿
            chosen.append((z, feat))           # (zå, ç‰¹å¾´å)
        if len(chosen) == 6:
            break

    # DataFrame åŒ–ã—ã¦ä¿å­˜
    top6_df = pd.DataFrame(chosen, columns=["latent_axis", "max_feature"])
    top6_df["abs_corr"] = top6_df.apply(lambda r: abs_corr.loc[r.latent_axis, r.max_feature], axis=1)

    top6_path = os.path.join(config.results_data_path, "top6_latent_axes.csv")
    top6_df.to_csv(top6_path, index=False)
    logger.info(f"Top-6 latent axes (dup-free) saved â†’ {top6_path}")

    # æ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ¬ãƒ¼ãƒ€ãƒ¼ã§ä½¿ãˆã‚‹å½¢ã«
    sel_idx = [int(z[1:]) for z in top6_df["latent_axis"]]
    
    svc_names = feat_df.index.tolist()
    
    # ---------------------------------------------------------
    # (æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹) ã¾ãš 6 è»¸ã ã‘æŠœãå‡ºã—ã¦ min / max ã‚’å–ã‚‹
    # ---------------------------------------------------------
    Z_exist_6 = z_existing[:, sel_idx]               # shape = (N_exist, 6)
    z_min     = Z_exist_6.min(axis=0)                # (6,)
    z_max     = Z_exist_6.max(axis=0)                # (6,)

    # ---------------------------------------------------------
    # (æ–°ã‚µãƒ¼ãƒ“ã‚¹) æ—¢å­˜ min-max ã§ç›¸å¯¾ã‚¹ã‚±ãƒ¼ãƒ«
    # ---------------------------------------------------------
    Z_new_6        = z_new[:, sel_idx]               # ç”Ÿã® z (N_new, 6)
    Z_new_scaled   = (Z_new_6 - z_min) / (z_max - z_min + 1e-8)
    # Z_new_scaled = np.clip(Z_new_scaled, r_low, r_high)

    # ---------------------------------------------------------
    # ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ â‘  : ã¯ã¿å‡ºã—ä½™è£•ã‚’è¨ˆç®—
    #   æ—¢å­˜åŸºæº–ã® 0ã€œ1 ã‚’è¶…ãˆã¦ã‚‚ â€œãã®ã¾ã¾â€ æã‘ã‚‹ã‚ˆã†ã«
    #   â–  r_low  : new ã®æœ€å°å€¤ (<=0 ãªã‚‰ãƒã‚¤ãƒŠã‚¹) âˆ’ ã¡ã‚‡ã„ä½™è£•
    #   â–  r_high : new ã®æœ€å¤§å€¤ (>=1 ãªã‚‰ 1è¶…)   ï¼‹ ã¡ã‚‡ã„ä½™è£•
    # ---------------------------------------------------------
    margin = 0.05
    r_low  = np.floor( (Z_new_scaled.min() - margin) * 10 ) / 10   # ä¾‹ãˆã° -0.2
    r_high = np.ceil ( (Z_new_scaled.max() + margin) * 10 ) / 10   # ä¾‹ãˆã°  1.3

    # ---------------------------------------------------------
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    # ---------------------------------------------------------
    radar_new_dir = os.path.join(config.results_data_path, "radar_new")
    os.makedirs(radar_new_dir, exist_ok=True)

    # ---------------------------------------------------------
    # 1 æšãšã¤æç”»ï¼ˆsave_radar_charts ã¯å‰å›ç¤ºã—ãŸæ±ç”¨ç‰ˆã‚’æƒ³å®šï¼‰
    #   â€¢ mins / maxs = None  â†’ ã‚‚ã†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã‚ã‚‹ã®ã§ä¸è¦
    #   â€¢ ylim        = (r_low, r_high)  â† ä¸‹é™ã‚‚ä¸Šé™ã‚‚å›ºå®š
    # ---------------------------------------------------------
    





    # ---------- PCAï¼ˆæ—¢å­˜ã§ fit, new ã¯ transform ã ã‘ï¼‰ ----------
    (score_df, load_df, var_ratio,
    scaler_exist, pca_exist) = run_pca(
            X_exist   = feat_df.values.astype(np.float32),
            X_new     = emb_new.astype(np.float32),
            feat_cols = feat_df.columns,
            svc_exist_names = feat_df.index.tolist(),
            out_dir  = config.results_data_path,
            n_components = 6,
            fit_on_exist_only = True,
            logger   = logger)

    # --- æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆ33â†’6 æ¬¡å…ƒã¸å¤‰æ›ï¼‰ ---
    Z_exist_pc = pca_exist.transform(
                    scaler_exist.transform(feat_df.values))[:, :6]

    # --- æ–°ã‚µãƒ¼ãƒ“ã‚¹ã‚‚ â€œtransformâ€ ã ã‘ ---
    Z_new_pc = pca_exist.transform(
                scaler_exist.transform(emb_new))[:, :6]

    # æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã ã‘ã§ min / max ã‚’æ±ºã‚ã‚‹
    pc_min = Z_exist_pc.min(axis=0)
    pc_max = Z_exist_pc.max(axis=0)

    # new ãŒ 0-1 ã‚’è¶…ãˆãŸã¨ãã«ä½™ç™½ã‚’ä»˜ã‘ã¦æã‘ã‚‹ã‚ˆã†ã«
    margin  = .05
    new_scaled = (Z_new_pc - pc_min) / (pc_max - pc_min + 1e-8)
    r_low  = np.floor((new_scaled.min() - margin)*10)/10   # ä¾‹: -0.2
    r_high = np.ceil ((new_scaled.max() + margin)*10)/10   # ä¾‹:  1.3


    # 0â€“1 ã‚¹ã‚±ãƒ¼ãƒ«
    mins = score_df.min();  maxs = score_df.max()
    score_scaled = (score_df - mins) / (maxs - mins + 1e-8)
    
    # æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹
    save_radar_charts(
        z_matrix = Z_exist_pc,
        svc_names= feat_df.index.tolist(),
        output_dir = os.path.join(config.results_data_path, "radar_pca_exist"),
        sel_idx  = list(range(6)),              # PC1â€¦PC6
        mins     = pc_min,
        maxs     = pc_max,
        ylim     = (0, 1.0)
    )

    # æ–°ã‚µãƒ¼ãƒ“ã‚¹
    save_radar_charts(
        z_matrix = Z_new_pc,
        svc_names= [f"new{i}" for i in range(Z_new_pc.shape[0])],
        output_dir = os.path.join(config.results_data_path, "radar_pca_new"),
        sel_idx    = list(range(6)),
        mins       = pc_min,                    # æ—¢å­˜ã¨åŒã˜
        maxs       = pc_max,
        ylim       = (r_low, r_high)            # ã¯ã¿å‡ºã—å¯¾å¿œ
    )

    # # # å¯è¦–åŒ–ã—ãŸã„ã‚µãƒ¼ãƒ“ã‚¹
    # # top_services = df_result["service"].tolist()

    # # save_radar_batch(score_scaled,
    # #                 sel_services = top_services,
    # #                 out_png = os.path.join(config.results_data_path,"pca_radar.png"),
    # #                 color_rule = lambda s: "firebrick" if s.startswith("new") else "steelblue")



    # # â˜… new ã‚µãƒ¼ãƒ“ã‚¹ã ã‘ã‚’å–ã‚Šå‡ºã—ã¦ã¯ã¿å‡ºã—ãƒ¬ãƒ³ã‚¸è¨ˆç®—
    # new_only  = score_df.loc[[s for s in score_df.index if s.startswith("new")]]
    # r_low_pc  = np.floor((new_only.min().min() - .05)*10)/10
    # r_high_pc = np.ceil ((new_only.max().max() + .05)*10)/10

    # save_radar_batch(score_df,
    #                 sel_services = score_df.index.tolist(),   # å…¨ã‚µãƒ¼ãƒ“ã‚¹å¯
    #                 out_png      = os.path.join(config.results_data_path,
    #                                             "pca_radar_all.png"),
    #                 color_rule   = lambda s: "crimson" if s.startswith("new")
    #                                             else "steelblue",
    #                 rmin=r_low_pc, rmax=r_high_pc)            # â† è¿½åŠ 


    # Optimal Transport
    logger.info("Starting Optimal Transportation analysis...")
    from src.Optimal_transportation.utils import load_and_scale_data, calculate_mass_vectors
    from src.Optimal_transportation.ot_main import run_ot_for_candidate
    from src.Optimal_transportation.config import OTConfig
    from src.Optimal_transportation.visualize import visualize_results



    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    X_curr, Y_fut = load_and_scale_data(config, logger)
    
    # è³ªé‡ãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
    a_vec = calculate_mass_vectors(feat_df, OTConfig, logger)
    
    # OTå®Ÿè¡Œ
    results = []
    for idx in range(Y_fut.shape[0]):
        res = run_ot_for_candidate(X_curr, Y_fut, idx, 
                                 mass_curr=a_vec[:-2],
                                 nonuser_mass=a_vec[-2],
                                 residual_mass=a_vec[-1])
        results.append(res)

    # çµæœã®ä¿å­˜
    df_result = pd.DataFrame(results)
    os.makedirs(config.results_data_path, exist_ok=True)
    df_result.to_csv(f"{config.results_data_path}/ot_results.csv", index=False)
    logger.info(f"Results saved to {config.results_data_path}/ot_results.csv")
   
    # çµæœã®å¯è¦–åŒ–ã¨æ–°ã‚µãƒ¼ãƒ“ã‚¹ã®ç‰¹å¾´é‡ä¿å­˜
    Y_fut_df = visualize_results(df_result, Y_fut, feat_df, config, logger)
 
 
    # VAEã®pcaã—ãŸãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä¿å­˜(df_resultã‚’otã§ä½œã£ã¦ã‚‹ã‹ã‚‰)
    # å¯è¦–åŒ–ã—ãŸã„ã‚µãƒ¼ãƒ“ã‚¹
    top_services = df_result["service"].tolist()

    save_radar_batch(score_scaled,
                    sel_services = top_services,
                    out_png = os.path.join(config.results_data_path,"pca_radar.png"),
                    color_rule = lambda s: "firebrick" if s.startswith("new") else "steelblue")
    
if __name__ == "__main__":
    args = get_args()
    config = Config()
                
    main(args=args, config=Config())
    
    